{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好瓜\n",
      "{'编号': {1: '是', 2: '是', 3: '是', 4: '是', 5: '是', 6: '是', 7: '是', 8: '是', 9: '否', 10: '否', 11: '否', 12: '否', 13: '否', 14: '否', 15: '否', 16: '否'}}\n",
      "分类精度： 70.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 定义 ID3 决策树算法\n",
    "def ID3(data, original_data, features, target_attribute_name, parent_node_class=None):\n",
    "    # 如果所有目标值相同，则返回这个值\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "    \n",
    "    # 如果数据集为空，返回父节点的目标值\n",
    "    elif len(data) == 0:\n",
    "        return parent_node_class\n",
    "    \n",
    "    # 如果特征集为空，则返回数据集中出现最频繁的目标值\n",
    "    elif len(features) == 0:\n",
    "        return np.unique(original_data[target_attribute_name])[np.argmax(np.unique(original_data[target_attribute_name], return_counts=True)[1])]\n",
    "    \n",
    "    # 构建决策树\n",
    "    else:\n",
    "        # 设置父节点的目标值\n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])]\n",
    "        \n",
    "        # 选择最佳分割特征\n",
    "        best_feature = get_best_feature(data, features, target_attribute_name)\n",
    "        \n",
    "        # 构建树\n",
    "        tree = {best_feature: {}}\n",
    "        \n",
    "        # 移除已选择的特征\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        # 递归构建树\n",
    "        for value in np.unique(data[best_feature]):\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            subtree = ID3(sub_data, original_data, features, target_attribute_name, parent_node_class)\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return tree\n",
    "\n",
    "# 获取最佳分割特征\n",
    "def get_best_feature(data, features, target_attribute_name):\n",
    "    # 计算信息增益\n",
    "    info_gain = []\n",
    "    for feature in features:\n",
    "        info_gain.append(calculate_information_gain(data, feature, target_attribute_name))\n",
    "    return features[np.argmax(info_gain)]\n",
    "\n",
    "# 计算信息增益\n",
    "def calculate_information_gain(data, split_attribute_name, target_attribute_name):\n",
    "    # 计算数据集的熵\n",
    "    total_entropy = calculate_entropy(data[target_attribute_name])\n",
    "    \n",
    "    # 计算分割属性的熵和信息增益\n",
    "    values, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
    "    weighted_entropy = 0\n",
    "    for i in range(len(values)):\n",
    "        subset = data.where(data[split_attribute_name] == values[i]).dropna()\n",
    "        weighted_entropy += (counts[i] / np.sum(counts)) * calculate_entropy(subset[target_attribute_name])\n",
    "    \n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "# 计算熵\n",
    "def calculate_entropy(target):\n",
    "    elements, counts = np.unique(target, return_counts=True)\n",
    "    entropy = -np.sum([(counts[i] / np.sum(counts)) * np.log2(counts[i] / np.sum(counts)) for i in range(len(elements))])\n",
    "    return entropy\n",
    "\n",
    "# 对测试集进行预测\n",
    "def predict(tree, test_data):\n",
    "    predictions = []\n",
    "    for index, row in test_data.iterrows():\n",
    "        prediction = classify(tree, row)\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "# 对单个样本进行分类\n",
    "def classify(node, sample):\n",
    "    if isinstance(node, dict):\n",
    "        key = list(node.keys())[0]\n",
    "        value = sample[key]\n",
    "        if value in node[key]:\n",
    "            return classify(node[key][value], sample)\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "    else:\n",
    "        return node\n",
    "\n",
    "# 读取数据集\n",
    "train_data = pd.read_csv('Watermelon-train1.csv',encoding=\"gb2312\")  # 读取训练集\n",
    "test_data = pd.read_csv('Watermelon-test1.csv',encoding=\"gb2312\")  # 读取测试集\n",
    "\n",
    "# 获取特征和目标列\n",
    "features = train_data.columns[:-1]\n",
    "target_attribute_name = train_data.columns[-1]\n",
    "print(target_attribute_name)\n",
    "\n",
    "# 构建决策树\n",
    "tree = ID3(train_data, train_data, features, target_attribute_name)\n",
    "print(tree)\n",
    "\n",
    "# 对测试集进行预测\n",
    "predictions = predict(tree, test_data)\n",
    "\n",
    "# 计算分类精度\n",
    "accuracy = np.mean(predictions == test_data[target_attribute_name]) * 100\n",
    "print(\"分类精度：\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#计算信息熵\n",
    "def cal_information_entropy(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    label_class =data_label.value_counts() #总共有多少类\n",
    "    Ent = 0\n",
    "    for k in label_class.keys():\n",
    "        p_k = label_class[k]/len(data_label)\n",
    "        Ent += -p_k*np.log2(p_k)\n",
    "    return Ent\n",
    "\n",
    "#计算给定数据属性a的信息增益\n",
    "def cal_information_gain(data, a):\n",
    "    Ent = cal_information_entropy(data)\n",
    "    feature_class = data[a].value_counts() #特征有多少种可能\n",
    "    gain = 0\n",
    "    for v in feature_class.keys():\n",
    "        weight = feature_class[v]/data.shape[0]\n",
    "        Ent_v = cal_information_entropy(data.loc[data[a] == v])\n",
    "        gain += weight*Ent_v\n",
    "    return Ent - gain\n",
    "\n",
    "def cal_gain_ratio(data , a):\n",
    "    #先计算固有值intrinsic_value\n",
    "    IV_a = 0\n",
    "    feature_class = data[a].value_counts()  # 特征有多少种可能\n",
    "    for v in feature_class.keys():\n",
    "        weight = feature_class[v]/data.shape[0]\n",
    "        IV_a += -weight*np.log2(weight)\n",
    "    gain_ration = cal_information_gain(data,a)/IV_a\n",
    "    return gain_ration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9975025463691153\n",
      "2 0.24403953873351492\n",
      "1 0.10812516526536531\n",
      "2 0.06843956584615814\n",
      "1 0.14267495956679288\n",
      "2 0.1017593980537369\n",
      "1 0.14078143361499584\n",
      "2 0.10562670944314426\n",
      "1 0.3805918973682686\n",
      "2 0.2630853587192754\n",
      "1 0.9975025463691153\n",
      "2 0.24403953873351492\n",
      "{'编号': {1: '是', 2: '是', 3: '是', 4: '是', 5: '是', 6: '是', 7: '是', 8: '是', 9: '否', 10: '否', 11: '否', 12: '否', 13: '否', 14: '否', 15: '否', 16: '否', 17: '否'}}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m test_data_1 \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39m色泽\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39m青绿\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m根蒂\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39m蜷缩\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m敲声\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39m浊响\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m纹理\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39m稍糊\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m脐部\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39m凹陷\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m触感\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39m硬滑\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m     70\u001b[0m test_data_2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mWatermelon-test2.csv\u001b[39m\u001b[39m'\u001b[39m,encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgb2312\u001b[39m\u001b[39m\"\u001b[39m) \n\u001b[1;32m---> 71\u001b[0m result \u001b[39m=\u001b[39m predict(dicision_Tree,test_data_2)\n\u001b[0;32m     72\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m分类结果为\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m好瓜\u001b[39m\u001b[39m'\u001b[39m\u001b[39mif\u001b[39;00m result \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m坏瓜\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 52\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(Tree, test_data)\u001b[0m\n\u001b[0;32m     50\u001b[0m second_dict \u001b[39m=\u001b[39m Tree[first_feature]\n\u001b[0;32m     51\u001b[0m input_first \u001b[39m=\u001b[39m test_data\u001b[39m.\u001b[39mget(first_feature)\n\u001b[1;32m---> 52\u001b[0m input_value \u001b[39m=\u001b[39m second_dict[input_first]\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(input_value , \u001b[39mdict\u001b[39m): \u001b[39m#判断分支还是不是字典\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     class_label \u001b[39m=\u001b[39m predict(input_value, test_data)\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'Series'"
     ]
    }
   ],
   "source": [
    "\n",
    "#获取标签最多的那一类\n",
    "def get_most_label(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    label_sort = data_label.value_counts(sort=True)\n",
    "    return label_sort.keys()[0]\n",
    "\n",
    "#挑选最优特征，即在信息增益大于平均水平的特征中选取增益率最高的特征\n",
    "def get_best_feature(data):\n",
    "    features = data.columns[:-1]\n",
    "    res = {}\n",
    "    for a in features:\n",
    "        temp = cal_information_gain(data, a)\n",
    "        print(\"1\",temp)\n",
    "        gain_ration = cal_gain_ratio(data,a)\n",
    "        print(\"2\",gain_ration)\n",
    "        res[a] = (temp,gain_ration)\n",
    "    res = sorted(res.items(),key=lambda x:x[1][0],reverse=True) #按信息增益排名\n",
    "    res_avg = sum([x[1][0] for x in res])/len(res) #信息增益平均水平\n",
    "    good_res = [x for x in res if x[1][0] >= res_avg] #选取信息增益高于平均水平的特征\n",
    "    result =sorted(good_res,key=lambda x:x[1][1],reverse=True) #将信息增益高的特征按照增益率进行排名\n",
    "    return result[0][0] #返回高信息增益中增益率最大的特征\n",
    "\n",
    "##将数据转化为（属性值：数据）的元组形式返回，并删除之前的特征列\n",
    "def drop_exist_feature(data, best_feature):\n",
    "    attr = pd.unique(data[best_feature])\n",
    "    new_data = [(nd, data[data[best_feature] == nd]) for nd in attr]\n",
    "    new_data = [(n[0], n[1].drop([best_feature], axis=1)) for n in new_data]\n",
    "    return new_data\n",
    "\n",
    "#创建决策树\n",
    "def create_tree(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    if len(data_label.value_counts()) == 1: #只有一类\n",
    "        return data_label.values[0]\n",
    "    if all(len(data[i].value_counts()) == 1 for i in data.iloc[:,:-1].columns): #所有数据的特征值一样，选样本最多的类作为分类结果\n",
    "        return get_most_label(data)\n",
    "    best_feature = get_best_feature(data) #根据信息增益得到的最优划分特征\n",
    "    Tree = {best_feature:{}} #用字典形式存储决策树\n",
    "    exist_vals = pd.unique(data[best_feature])  # 当前数据下最佳特征的取值\n",
    "    if len(exist_vals) != len(column_count[best_feature]):  # 如果特征的取值相比于原来的少了\n",
    "        no_exist_attr = set(column_count[best_feature]) - set(exist_vals)  # 少的那些特征\n",
    "        for no_feat in no_exist_attr:\n",
    "            Tree[best_feature][no_feat] = get_most_label(data)  # 缺失的特征分类为当前类别最多的\n",
    "    for item in drop_exist_feature(data,best_feature): #根据特征值的不同递归创建决策树\n",
    "        Tree[best_feature][item[0]] = create_tree(item[1])\n",
    "    return Tree\n",
    "\n",
    "def predict(Tree , test_data):\n",
    "    first_feature = list(Tree.keys())[0]\n",
    "    second_dict = Tree[first_feature]\n",
    "    input_first = test_data.get(first_feature)\n",
    "    input_value = second_dict[input_first]\n",
    "    if isinstance(input_value , dict): #判断分支还是不是字典\n",
    "        class_label = predict(input_value, test_data)\n",
    "    else:\n",
    "        class_label = input_value\n",
    "    return class_label\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #读取数据\n",
    "    data = pd.read_csv('Watermelon-train2.csv',encoding=\"gb2312\")\n",
    "    # 统计每个特征的取值情况作为全局变量\n",
    "    column_count = dict([(ds, list(pd.unique(data[ds]))) for ds in data.iloc[:, :-1].columns])\n",
    "\n",
    "    #创建决策树\n",
    "    dicision_Tree = create_tree(data)\n",
    "    print(dicision_Tree)\n",
    "    #测试数据\n",
    "    test_data_1 = {'色泽':'青绿','根蒂':'蜷缩','敲声':'浊响','纹理':'稍糊','脐部':'凹陷','触感':'硬滑'}\n",
    "    test_data_2 = pd.read_csv('Watermelon-test2.csv',encoding=\"gb2312\") \n",
    "    result = predict(dicision_Tree,test_data_2)\n",
    "    print('分类结果为'+'好瓜'if result == 1 else '坏瓜')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
